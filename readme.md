ğŸ›’ Walmart Sales Data Engineering Pipeline (End-to-End ETL)
ğŸ” Overview

This project implements a production-style batch ETL pipeline for Walmart sales data using PySpark and Databricks, following the Medallion (Bronzeâ€“Silverâ€“Gold) architecture.
The pipeline ingests raw sales data, applies data quality checks and transformations, and produces analytics-ready datasets for business reporting.

ğŸ¯ Business Problem

Retail organizations need reliable, scalable, and cost-efficient pipelines to analyze sales performance across stores, products, and time periods.

Challenges addressed:

Large-volume data processing

Data quality and consistency

Incremental processing

Analytics-ready outputs for BI teams

ğŸ§  Solution Approach

Designed a layered ETL architecture (Bronze â†’ Silver â†’ Gold)

Implemented incremental processing using Spark optimizations

Applied data validation rules to ensure correctness

Optimized Spark jobs for performance and cost

ğŸ— Architecture
Source Data
   â”‚
   â–¼
Bronze Layer (Raw Ingestion)
   â”‚  - Schema enforcement
   â”‚  - Raw data storage
   â–¼
Silver Layer (Cleaned & Enriched)
   â”‚  - Null handling
   â”‚  - Type casting
   â”‚  - Business rules
   â–¼
Gold Layer (Analytics Ready)
   - Aggregated sales metrics
   - KPIs for reporting

ğŸ›  Tech Stack

Language: Python

Processing: PySpark

Platform: Databricks

Storage: Delta Lake

Architecture: Medallion (Bronzeâ€“Silverâ€“Gold)

ğŸš€ Pipeline Features

Batch ETL with scalable Spark jobs

Schema validation & null checks

Repartitioning and caching for performance

Idempotent transformations

Analytics-ready fact tables

ğŸ“Š Example KPIs Produced

Total sales per store

Weekly & monthly revenue trends

Product-level performance

Store-wise contribution to revenue

âš¡ Performance Optimization

Optimized joins using broadcast hints

Reduced shuffles via proper partitioning

Cached intermediate datasets

Used column pruning & predicate pushdown

ğŸ§ª Data Quality Checks

Schema validation

Null & duplicate detection

Row count reconciliation between layers

ğŸ“ˆ Results

Processed large-scale sales data efficiently

Reduced transformation runtime through Spark tuning

Delivered clean, analytics-ready datasets for BI consumption

ğŸ”® Future Improvements

Airflow orchestration

Incremental loads using Delta Lake MERGE

Automated data quality alerts

BI dashboard integration

ğŸ‘¤ Author

Tushar Patgar
Data Engineer | PySpark | Databricks | AWS
